import os
import typing
import logging

from dotenv import load_dotenv, find_dotenv

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# pip install -U wikipedia

from langchain.agents.agent_toolkits import create_python_agent
from langchain.agents import load_tools, initialize_agent
from langchain.agents import AgentType
from langchain.tools.python.tool import PythonREPLTool
from langchain.python import PythonREPL
from langchain.chat_models import ChatOpenAI

from langchain.tools import tool

logging.basicConfig(level=logging.DEBUG)

LOGGER = logging.getLogger(__file__)

_ = load_dotenv(find_dotenv())


# account for deprecation of LLM model
import datetime

# Get the current date
current_date = datetime.datetime.now().date()

# Define the date after which the model should be set to "gpt-3.5-turbo"
target_date = datetime.date(2024, 6, 12)

# Set the model variable based on the current date
if current_date > target_date:
    llm_model = "gpt-3.5-turbo"
else:
    llm_model = "gpt-3.5-turbo-0301"

import requests
from pydantic import BaseModel, Field
import datetime

# Define the input schema
class OpenMeteoInput(BaseModel):
    latitude: float = Field(
        ..., description="Latitude of the location to fetch weather data for"
    )
    longitude: float = Field(
        ..., description="Longitude of the location to fetch weather data for"
    )


@tool(args_schema=OpenMeteoInput)
def get_current_temperature(latitude: float, longitude: float) -> dict:
    """Fetch current temperature for given coordinates."""

    BASE_URL = "https://api.open-meteo.com/v1/forecast"

    # Parameters for the request
    params = {
        "latitude": latitude,
        "longitude": longitude,
        "hourly": "temperature_2m",
        "forecast_days": 1,
    }

    # Make the request
    response = requests.get(BASE_URL, params=params)

    if response.status_code == 200:
        results = response.json()
    else:
        raise Exception(f"API Request failed with status code: {response.status_code}")

    current_utc_time = datetime.datetime.utcnow()
    time_list = [
        datetime.datetime.fromisoformat(time_str.replace("Z", "+00:00"))
        for time_str in results["hourly"]["time"]
    ]
    temperature_list = results["hourly"]["temperature_2m"]

    closest_time_index = min(
        range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time)
    )
    current_temperature = temperature_list[closest_time_index]

    return f"The current temperature is {current_temperature}°C"


import wikipedia


@tool
def search_wikipedia(query: str) -> str:
    """Run Wikipedia search and get page summaries."""
    page_titles = wikipedia.search(query)
    summaries = []
    for page_title in page_titles[:3]:
        try:
            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)
            summaries.append(f"Page: {page_title}\nSummary: {wiki_page.summary}")
        except (
            self.wiki_client.exceptions.PageError,
            self.wiki_client.exceptions.DisambiguationError,
        ):
            pass
    if not summaries:
        return "No good Wikipedia Search Result was found"
    return "\n\n".join(summaries)


tools = [get_current_temperature, search_wikipedia]

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.tools.render import format_tool_to_openai_function
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser

functions = [format_tool_to_openai_function(f) for f in tools]
model = ChatOpenAI(temperature=0).bind(functions=functions)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are helpful but sassy assistant"),
        ("user", "{input}"),
    ]
)
chain = prompt | model | OpenAIFunctionsAgentOutputParser()

result = chain.invoke({"input": "what is the weather is sf?"})

result.tool

result.tool_input

from langchain.prompts import MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are helpful but sassy assistant"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

chain = prompt | model | OpenAIFunctionsAgentOutputParser()

result1 = chain.invoke({"input": "what is the weather is sf?", "agent_scratchpad": []})

result1.tool

observation = get_current_temperature(result1.tool_input)

observation

type(result1)

from langchain.agents.format_scratchpad import format_to_openai_functions

result1.message_log


format_to_openai_functions(
    [
        (result1, observation),
    ]
)

result2 = chain.invoke(
    {
        "input": "what is the weather is sf?",
        "agent_scratchpad": format_to_openai_functions([(result1, observation)]),
    }
)

result2

from langchain.schema.agent import AgentFinish


def run_agent(user_input):
    intermediate_steps = []
    while True:
        result = chain.invoke(
            {
                "input": user_input,
                "agent_scratchpad": format_to_openai_functions(intermediate_steps),
            }
        )
        if isinstance(result, AgentFinish):
            return result
        tool = {
            "search_wikipedia": search_wikipedia,
            "get_current_temperature": get_current_temperature,
        }[result.tool]
        observation = tool.run(result.tool_input)
        intermediate_steps.append((result, observation))


from langchain.schema.runnable import RunnablePassthrough

agent_chain = (
    RunnablePassthrough.assign(
        agent_scratchpad=lambda x: format_to_openai_functions(x["intermediate_steps"])
    )
    | chain
)


def run_agent(user_input):
    intermediate_steps = []
    while True:
        result = agent_chain.invoke(
            {"input": user_input, "intermediate_steps": intermediate_steps}
        )
        if isinstance(result, AgentFinish):
            return result
        tool = {
            "search_wikipedia": search_wikipedia,
            "get_current_temperature": get_current_temperature,
        }[result.tool]
        observation = tool.run(result.tool_input)
        intermediate_steps.append((result, observation))


run_agent("what is the weather is sf?")


run_agent("what is langchain?")

run_agent("hi!")

from langchain.agents import AgentExecutor

agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)


agent_executor.invoke({"input": "what is langchain?"})

agent_executor.invoke({"input": "my name is bob"})

agent_executor.invoke({"input": "what is my name"})

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are helpful but sassy assistant"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

agent_chain = (
    RunnablePassthrough.assign(
        agent_scratchpad=lambda x: format_to_openai_functions(x["intermediate_steps"])
    )
    | prompt
    | model
    | OpenAIFunctionsAgentOutputParser()
)

from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")

agent_executor = AgentExecutor(
    agent=agent_chain, tools=tools, verbose=True, memory=memory
)

agent_executor.invoke({"input": "my name is bob"})

agent_executor.invoke({"input": "whats my name"})

agent_executor.invoke({"input": "whats the weather in sf?"})


## Create a Chatbot
##


@tool
def create_your_own(query: str) -> str:
    """This function can do whatever you would like once you fill it in"""
    print(type(query))
    return query[::-1]


tools = [get_current_temperature, search_wikipedia, create_your_own]

import panel as pn  # GUI

pn.extension()
import panel as pn
import param


class cbfs(param.Parameterized):
    def __init__(self, tools, **params):
        super(cbfs, self).__init__(**params)
        self.panels = []
        self.functions = [format_tool_to_openai_function(f) for f in tools]
        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)
        self.memory = ConversationBufferMemory(
            return_messages=True, memory_key="chat_history"
        )
        self.prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "You are helpful but sassy assistant"),
                MessagesPlaceholder(variable_name="chat_history"),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad"),
            ]
        )
        self.chain = (
            RunnablePassthrough.assign(
                agent_scratchpad=lambda x: format_to_openai_functions(
                    x["intermediate_steps"]
                )
            )
            | self.prompt
            | self.model
            | OpenAIFunctionsAgentOutputParser()
        )
        self.qa = AgentExecutor(
            agent=self.chain, tools=tools, verbose=False, memory=self.memory
        )

    def convchain(self, query):
        if not query:
            return
        inp.value = ""
        result = self.qa.invoke({"input": query})
        self.answer = result["output"]
        self.panels.extend(
            [
                pn.Row("User:", pn.pane.Markdown(query, width=450)),
                pn.Row(
                    "ChatBot:",
                    pn.pane.Markdown(
                        self.answer, width=450, styles={"background-color": "#F6F6F6"}
                    ),
                ),
            ]
        )
        return pn.WidgetBox(*self.panels, scroll=True)

    def clr_history(self, count=0):
        self.chat_history = []
        return


cb = cbfs(tools)

inp = pn.widgets.TextInput(placeholder="Enter text here…")

conversation = pn.bind(cb.convchain, inp)

tab1 = pn.Column(
    pn.Row(inp),
    pn.layout.Divider(),
    pn.panel(conversation, loading_indicator=True, height=400),
    pn.layout.Divider(),
)

dashboard = pn.Column(
    pn.Row(pn.pane.Markdown("# QnA_Bot")), pn.Tabs(("Conversation", tab1))
)
dashboard
